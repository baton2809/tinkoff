–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å —è –∏–∑—É—á–∏–ª –≤–µ—Å—å –≤–∞—à –ø—Ä–æ–µ–∫—Ç –∏ –≥–æ—Ç–æ–≤ –æ–±—ä—è—Å–Ω–∏—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –∑–∞—â–∏—Ç–µ. –≠—Ç–æ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é Reward Model –¥–ª—è –∞–ª–∞–π–Ω–º–µ–Ω—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º –≤—Å–µ –ø–æ –ø–æ—Ä—è–¥–∫—É:

## üéØ **–û–±—â–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞**

–í–∞—à –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç **Level 1** —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã Alignment –≤ –¢–∏–Ω—å–∫–æ—Ñ—Ñ. –¶–µ–ª—å - –æ–±—É—á–∏—Ç—å **Reward Model** –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ **REINFORCE with baseline** –¥–ª—è –∞–ª–∞–π–Ω–º–µ–Ω—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

## üìä **–ß—Ç–æ —Ç–∞–∫–æ–µ Reward Model –∏ –∑–∞—á–µ–º –æ–Ω–∞ –Ω—É–∂–Ω–∞**

**Reward Model (–º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è)** - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–∞—Ä—É "–ø—Ä–æ–º–ø—Ç + –æ—Ç–≤–µ—Ç" –∏ –≤—ã–¥–∞–µ—Ç —á–∏—Å–ª–æ–≤—É—é –æ—Ü–µ–Ω–∫—É (reward score) - –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à —ç—Ç–æ—Ç –æ—Ç–≤–µ—Ç.

**–ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ?** –í RLHF (Reinforcement Learning from Human Feedback) –Ω–∞–º –Ω—É–∂–µ–Ω —Å–ø–æ—Å–æ–± –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, —á—Ç–æ–±—ã –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª—É—á—à–∏–µ –æ—Ç–≤–µ—Ç—ã.

## üóÉÔ∏è **–î–∞—Ç–∞—Å–µ—Ç HelpSteer2_binarized**

–í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç `esfrankel17/HelpSteer2_binarized` (–≤ –∫–æ–¥–µ –µ—Å—Ç—å —Ç–∞–∫–∂–µ –≤–∞—Ä–∏–∞–Ω—Ç `juyoungml/HelpSteer2-binarized`). –≠—Ç–æ **–±–∏–Ω–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è** –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ HelpSteer2.

**–ß—Ç–æ –∑–Ω–∞—á–∏—Ç "–±–∏–Ω–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π"?** –ò—Å—Ö–æ–¥–Ω—ã–π HelpSteer2 —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏ —Ç.–¥.). –ë–∏–Ω–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —ç—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –≤ –ø–∞—Ä—ã "—Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç" vs "–ø–ª–æ—Ö–æ–π –æ—Ç–≤–µ—Ç".

**–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö:**
```python
{
    "prompt": "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
    "chosen": "–•–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç", 
    "rejected": "–ü–ª–æ—Ö–æ–π –æ—Ç–≤–µ—Ç"
}
```

**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:**
- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: 8,678 –ø—Ä–∏–º–µ—Ä–æ–≤
- Train: 6,942 –ø—Ä–∏–º–µ—Ä–∞ (80%)
- Validation: 1,736 –ø—Ä–∏–º–µ—Ä–æ–≤ (20%)
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 256 —Ç–æ–∫–µ–Ω–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)

## ü§ñ **SFT –º–æ–¥–µ–ª—å (Supervised Fine-Tuning)**

–í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ `HuggingFaceTB/SmolLM2-135M-Instruct` –∫–∞–∫ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å.

**–ß—Ç–æ —Ç–∞–∫–æ–µ SFT?** Supervised Fine-Tuning - —ç—Ç–æ —ç—Ç–∞–ø, –≥–¥–µ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞—é—Ç –Ω–∞ –ø–∞—Ä–∞—Ö "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–æ—Ç–≤–µ—Ç", —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–∞—É—á–∏–ª–∞—Å—å —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. SmolLM2-135M-Instruct —É–∂–µ –ø—Ä–æ—à–ª–∞ —ç—Ç–æ—Ç —ç—Ç–∞–ø.

**–ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–∞ –º–æ–¥–µ–ª—å?** –û–Ω–∞ –Ω–µ–±–æ–ª—å—à–∞—è (135M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —É–∂–µ –æ–±—É—á–µ–Ω–∞ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.

## üèÜ **RewardTrainer –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ TRL**

**TRL (Transformer Reinforcement Learning)** - —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ—Ç Hugging Face –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.

**RewardTrainer** - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reward models. –û–Ω:
1. –ë–µ—Ä–µ—Ç –ø–∞—Ä—ã (chosen, rejected) 
2. –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∏—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
3. –í—ã—á–∏—Å–ª—è–µ—Ç loss —Ç–∞–∫, —á—Ç–æ–±—ã chosen –ø–æ–ª—É—á–∞–ª –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π score, —á–µ–º rejected
4. –û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏

**–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:** –û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ranking loss –∏–ª–∏ contrastive loss, –∫–æ—Ç–æ—Ä–∞—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É scores –¥–ª—è chosen –∏ rejected –æ—Ç–≤–µ—Ç–æ–≤.

## üìà **Moving Average (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)**

–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ REINFORCE with baseline, **moving average** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ **baseline** –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

**–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç?** 
- –í REINFORCE –º—ã –æ–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ reward'—É
- –ù–æ reward'—ã –º–æ–≥—É—Ç —Å–∏–ª—å–Ω–æ –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º
- Baseline (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö reward'–æ–≤) –≤—ã—á–∏—Ç–∞–µ—Ç—Å—è –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ reward'–∞
- –≠—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –Ω–µ –º–µ–Ω—è—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞

## üîß **–ê–ª–∞–π–Ω–º–µ–Ω—Ç—ã (Alignment)**

**Alignment** - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ –ø–æ–ª–µ–∑–Ω—ã–µ, –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º –æ—Ç–≤–µ—Ç—ã.

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**
1. **RLHF** (Reinforcement Learning from Human Feedback) - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ ChatGPT
2. **REINFORCE** - –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ PPO, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –≤ —Å—Ç–∞—Ç—å–µ "Back to Basics"

## üöÄ **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∞—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è**

1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö** (`load_dataset.py`):
   - –ó–∞–≥—Ä—É–∑–∫–∞ HelpSteer2_binarized
   - –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation (80/20)
   - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã

2. **–û–±—É—á–µ–Ω–∏–µ Reward Model** (`train_reward_model.py`):
   - –ó–∞–≥—Ä—É–∑–∫–∞ SmolLM2-135M-Instruct
   - –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è sequence classification (1 –≤—ã—Ö–æ–¥)
   - –û–±—É—á–µ–Ω–∏–µ —Å RewardTrainer
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Apple Silicon (MPS)

3. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** (`test_reward_model.py`):
   - –ö–ª–∞—Å—Å RewardModelEvaluator
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
   - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏

## üí° **–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è**

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏:**
- Batch size = 1 —Å gradient accumulation = 8
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ = 128 —Ç–æ–∫–µ–Ω–æ–≤
- –û—Ç–∫–ª—é—á–µ–Ω–∏–µ FP16 –¥–ª—è MPS (Apple Silicon)
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

**–ü–æ—á–µ–º—É –Ω–µ RLOOTrainer?** –í –∑–∞–¥–∞–Ω–∏–∏ —Å–∫–∞–∑–∞–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RLOOTrainer, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–≥–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤ —Å—Ç–∞—Ç—å–µ "Back to Basics". RLOOTrainer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é —Å—Ö–µ–º—É baseline'–∞.

## üéØ **–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ (REINFORCE)**

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è Reward Model —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø - —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è REINFORCE with baseline:
1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SFT –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
2. –û—Ü–µ–Ω–∏–≤–∞—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é Reward Model
3. –û–±–Ω–æ–≤–ª—è—Ç—å SFT –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é REINFORCE
4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å moving average –∫–∞–∫ baseline

–≠—Ç–æ –æ—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞! –í—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–ª–∞–π–Ω–º–µ–Ω—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.